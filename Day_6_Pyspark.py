
"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   DAY 6: PYSPARK - DISTRIBUTED COMPUTING                       â•‘
â•‘                                                                                â•‘
â•‘  Learning Objectives:                                                          â•‘
â•‘  1. Set up PySpark environment and handle Windows compatibility                â•‘
â•‘  2. Create Spark Session and initialize Spark context                          â•‘
â•‘  3. Create and work with Spark DataFrames                                      â•‘
â•‘  4. Perform distributed data processing operations                             â•‘
â•‘  5. Understand lazy evaluation and Spark execution model                       â•‘
â•‘                                                                                â•‘
â•‘  Key Concepts:                                                                 â•‘
â•‘  - PySpark: Python API for Apache Spark                                        â•‘
â•‘  - SparkSession: Entry point for Spark functionality                           â•‘
â•‘  - Spark DataFrame: Distributed collection of rows with named columns          â•‘
â•‘  - RDD: Resilient Distributed Dataset (lower-level abstraction)                â•‘
â•‘  - Lazy Evaluation: Transformations not executed until action called            â•‘
â•‘  - Distributed Computing: Data split across multiple nodes/partitions          â•‘
â•‘                                                                                â•‘
â•‘  Prerequisites:                                                                â•‘
â•‘  - Python 3.6+ installed                                                       â•‘
â•‘  - Java JDK 8 or 11 installed                                                  â•‘
â•‘  - JAVA_HOME environment variable set correctly                                â•‘
â•‘  - PySpark installed: pip install pyspark                                      â•‘
â•‘  - findspark installed: pip install findspark                                  â•‘
â•‘                                                                                â•‘
â•‘  Windows-Specific Setup:                                                       â•‘
â•‘  - Need to patch socketserver for Windows compatibility                        â•‘
â•‘  - findspark helps locate Spark installation automatically                     â•‘
â•‘                                                                                â•‘
â•‘  Use Case:                                                                     â•‘
â•‘  Processing large datasets (TB/PB scale) across distributed cluster,          â•‘
â•‘  performing data transformations, aggregations in parallel                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import sys
import socketserver

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# WINDOWS COMPATIBILITY PATCH
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("=" * 80)
print("SETUP: Windows Compatibility Patch")
print("=" * 80)
print("\nExplanation:")
print("- Spark uses Unix sockets on Linux, but Windows doesn't support them")
print("- We replace UnixStreamServer with TCPServer for Windows compatibility")
print("- UnixStreamHandler â†’ StreamRequestHandler (handles network communication)\n")

if sys.platform == "win32":
    socketserver.UnixStreamServer = socketserver.TCPServer
    socketserver.UnixStreamHandler = socketserver.StreamRequestHandler
    print("âœ… Windows compatibility patch applied")
else:
    print("âœ… Running on Unix/Linux (no patch needed)")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# PYSPARK SETUP
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\n" + "=" * 80)
print("SETUP: PySpark Initialization")
print("=" * 80)
print("\nExplanation:")
print("- findspark.init() locates Spark installation automatically")
print("- Adds Spark libraries to Python path")
print("- Removes need to manually set SPARK_HOME\n")

import findspark
findspark.init()
print("âœ… findspark initialized")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CREATE SPARK SESSION
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\n" + "=" * 80)
print("EXERCISE 1: Create Spark Session")
print("=" * 80)
print("\nExplanation:")
print("- SparkSession is the entry point for all Spark functionality")
print("- .builder: Fluent API for configuration")
print("- .appName(): Name of your Spark application (appears in Spark UI)")
print("- .getOrCreate(): Reuse existing session or create new one\n")

from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("EnvTest").getOrCreate()

print("âœ… SparkSession created:")
#print(f"   App Name: {spark.appName}")
print(f"   Spark Version: {spark.version}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# EXERCISE 2: Create DataFrame from List
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\n" + "=" * 80)
print("EXERCISE 2: Create Spark DataFrame from List")
print("=" * 80)
print("\nExplanation:")
print("- createDataFrame() creates a Spark DataFrame from data")
print("- Data: List of tuples (rows)")
print("- Schema: Column names as list")
print("- DataFrame: Distributed table with rows and columns")
print("- Similar to Pandas DataFrame but distributed across cluster\n")

print("Code Example:")
print("""
data = [("Rohit", 1), ("Divya", 2)]
df = spark.createDataFrame(data, ["Name", "Id"])
df.show()

Output:
+-----+---+
| Name| Id|
+-----+---+
|Rohit|  1|
|Divya|  2|
+-----+---+
""")

data = [("Rohit", 1), ("Divya", 2)]
df = spark.createDataFrame(data, ["Name", "Id"])

print("Spark DataFrame created:")
df.show()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# EXERCISE 3: DataFrame Operations
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\n" + "=" * 80)
print("EXERCISE 3: DataFrame Operations")
print("=" * 80)

# 3.1 Print Schema
print("\n3.1 - Print DataFrame Schema")
print("â”€" * 40)
print("Explanation:")
print("- printSchema() shows the structure of DataFrame")
print("- Column names and data types\n")
df.printSchema()

# 3.2 Count rows
print("\n3.2 - Count Rows")
print("â”€" * 40)
print("Explanation:")
print("- count() returns total number of rows")
print("- Triggers actual computation (action)\n")
row_count = df.count()
print(f"Total rows: {row_count}")

# 3.3 Display with show()
print("\n3.3 - Display Data with show()")
print("â”€" * 40)
print("Explanation:")
print("- show() displays top 20 rows (default)")
print("- Pretty-printed table format")
print("- Useful for verification\n")
print("Displaying all data:")
df.show(truncate=False)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# EXERCISE 4: Create DataFrame from Tuples (Sales Data)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\n" + "=" * 80)
print("EXERCISE 4: DataFrame with Multiple Columns (Sales Data)")
print("=" * 80)
print("\nExplanation:")
print("- Create more realistic data with multiple attributes")
print("- Each tuple represents one row")
print("- Column names define the schema")
print("- Data suitable for analysis and aggregation\n")

sales_data = [
    ("Rohit", "North", 12000),
    ("Divya", "South", 18000),
    ("Amit", "East", 10000),
    ("Rohit", "North", 15000),
    ("Divya", "South", 45000)
]

sales_schema = ["Name", "Region", "Sales"]

sales_df = spark.createDataFrame(sales_data, sales_schema)

print("Sales DataFrame:")
sales_df.show()
sales_df.printSchema()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# EXERCISE 5: Aggregations
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\n" + "=" * 80)
print("EXERCISE 5: Aggregation Operations")
print("=" * 80)

# 5.1 Group by and Sum
print("\n5.1 - GroupBy and Sum")
print("â”€" * 40)
print("Explanation:")
print("- groupBy(): Groups data by column")
print("- sum(): Aggregates numeric columns")
print("- Result: Total sales per region\n")

from pyspark.sql.functions import sum as spark_sum

region_summary = sales_df.groupBy("Region").agg(spark_sum("Sales").alias("TotalSales"))
print("Total Sales per Region:")
region_summary.show()

# 5.2 Multiple aggregations
print("\n5.2 - Multiple Aggregations")
print("â”€" * 40)
print("Explanation:")
print("- agg() with multiple functions")
print("- sum(): Total sales")
print("- count(): Number of transactions")
print("- Result: Comprehensive summary\n")

from pyspark.sql.functions import count as spark_count

summary = sales_df.groupBy("Name").agg(
    spark_sum("Sales").alias("TotalSales"),
    spark_count("*").alias("TransactionCount")
)
print("Summary by Name:")
summary.show()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# EXERCISE 6: Filtering
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\n" + "=" * 80)
print("EXERCISE 6: Filtering Data")
print("=" * 80)
print("\nExplanation:")
print("- filter() / where() select rows matching condition")
print("- Similar to WHERE clause in SQL")
print("- Returns new DataFrame with filtered rows\n")

print("Sales > 15000:")
high_sales = sales_df.filter(sales_df["Sales"] > 15000)
high_sales.show()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# EXERCISE 7: Select Specific Columns
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\n" + "=" * 80)
print("EXERCISE 7: Select Columns")
print("=" * 80)
print("\nExplanation:")
print("- select() chooses specific columns")
print("- Returns DataFrame with only selected columns\n")

selected = sales_df.select("Name", "Sales")
print("Selected Columns (Name, Sales):")
selected.show()

print("\n" + "=" * 80)
print("SUMMARY - PySpark Concepts")
print("=" * 80)
print("""
âœ… Core Concepts Covered:

1. SETUP:
   - Windows socket compatibility patch
   - findspark for automatic Spark discovery
   - SparkSession initialization
   
2. DATAFRAME CREATION:
   - From Python lists/tuples
   - With explicit schema (column names)
   - Distributed across cluster
   
3. OPERATIONS:
   - show(): Display data
   - printSchema(): Column structure
   - count(): Row count
   
4. TRANSFORMATIONS:
   - groupBy(): Group rows
   - filter(): Select rows by condition
   - select(): Choose columns
   - agg(): Aggregate functions (sum, count, avg, etc.)
   
5. KEY DIFFERENCES FROM PANDAS:
   - Distributed: Data split across nodes
   - Lazy Evaluation: Transformations not executed immediately
   - Action vs Transformation: Only actions trigger computation
   - Scalability: Can handle TB/PB of data

ðŸŽ¯ Transformations (Lazy - Cached, not executed):
   â”œâ”€â”€ select()
   â”œâ”€â”€ filter() / where()
   â”œâ”€â”€ groupBy() / agg()
   â”œâ”€â”€ join()
   â”œâ”€â”€ sort()
   â”œâ”€â”€ map() / flatMap()
   â””â”€â”€ union() / subtract()

âš¡ Actions (Trigger Execution):
   â”œâ”€â”€ show(): Display results
   â”œâ”€â”€ count(): Count rows
   â”œâ”€â”€ collect(): Get all data to driver
   â”œâ”€â”€ first(): Get first row
   â”œâ”€â”€ take(n): Get first n rows
   â”œâ”€â”€ saveAsTextFile(): Write to file
   â””â”€â”€ foreachRDD(): Process each partition

ðŸ’¡ Performance Tips:
   - Use filter early to reduce data
   - Select only needed columns
   - Partition data for distributed processing
   - Cache frequently used DataFrames (.cache())
   - Use Spark SQL for complex queries
   
ðŸš€ Advanced Topics (Beyond Scope):
   - SparkSQL and Catalyst optimizer
   - MLlib for machine learning
   - Streaming with Spark
   - Graph processing with GraphX
""")

print("\n" + "=" * 80)
print("âœ… All exercises completed!")
print("=" * 80)

###Read CSV and aggregate sales by region
from pyspark.sql.functions import sum

df=spark.read.csv("sales.csv",header=True,inferSchema=True)
df.show()

df_grouped = df.groupBy("Region").agg(sum("Sales").alias("TotalSales"))
df_sorted=df_grouped.orderBy(df_grouped.TotalSales.desc())
df_sorted.show()

